{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pyaudio\n",
    "import librosa\n",
    "import asyncio\n",
    "import threading\n",
    "#import deepspeech\n",
    "import wave\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoProcessor, AutoModelForCTC, TFWav2Vec2ForCTC, \\\n",
    "                         AutoTokenizer, AutoFeatureExtractor, T5ForConditionalGeneration, T5Tokenizer\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "#from autocorrect import Speller\n",
    "import openai\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO когда санкции снимут\n",
    "# import os\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./google_cred.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРЕДИСЛОВИЕ\n",
    "\n",
    "#### Перед тем, как запускать прогу, необходимо:\n",
    "\n",
    "1) установить virtual audio cable\n",
    "\n",
    "2) в sound mixer options для всех бырать defaul input = Linme 1(virtual audio cable)\n",
    "\n",
    "3) затем для нужной программы (окна) в oputput выбрать тот же кабель (Line 1, например) \n",
    "\n",
    "4) вызвать код для нахождения подключённых аудиоустройств: ```for i in range(a.recorder.get_device_count()):\n",
    "       print(i, a.recorder.get_device_info_by_index(i))```\n",
    "\n",
    "5) выбрать подходящее аудиоустройство (для записи должно быть устройство Line 1, у которого ```'maxInputChannels' != 0```,\n",
    "  а для вывода должно быть устройство Line 1, у которого ```'maxOutputChannels' != 0```\n",
    "\n",
    "6) и когда открываете потоки в kwargs передаёте нужный вам индекс в переменную ```input_device_index```\n",
    "\n",
    "7) после запуска у нас звук будет перенаправляться так: приложение->Line_1_output->Line_1_input->Наша прога."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioChanel():\n",
    "    def __init__(self):\n",
    "        #change this according to your model\n",
    "        self.chunk = 1024\n",
    "        self.format = pyaudio.paInt16\n",
    "        self.channels = 1#2\n",
    "        self.rate = 16000\n",
    "        self.recorder = pyaudio.PyAudio()\n",
    "        self.frames = []\n",
    "        self.seconds = 2\n",
    "        # Открыть поток для записи\n",
    "        #1 {'index': 1, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 2, \n",
    "        #   'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, \n",
    "        #   'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
    "        self.input_stream = self.recorder.open(format=self.format,\n",
    "                                          channels=self.channels,\n",
    "                                          rate=self.rate,\n",
    "                                          input=True,\n",
    "                                          frames_per_buffer=self.chunk,\n",
    "                                          input_device_index=1) #0,1 - work\n",
    "        # Открыть поток для воспроизведения\n",
    "        #5 {'index': 5, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 0, \n",
    "        #   'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, \n",
    "        #   'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
    "        self.output_stream = self.recorder.open(format=self.format,\n",
    "                                           channels=self.channels,\n",
    "                                           rate=self.rate,\n",
    "                                           output=True,\n",
    "                                           frames_per_buffer=self.chunk,\n",
    "                                           input_device_index=5) #5- work\n",
    "        \n",
    "        \n",
    "    def record(self):\n",
    "        while True:\n",
    "            data = self.input_stream.read(self.chunk)\n",
    "            self.frames.append(data)\n",
    "\n",
    "    def play(self, audio = None):\n",
    "        output_stream = self.recorder.open(format=self.format,\n",
    "                                           channels=self.channels,\n",
    "                                           rate=self.rate,\n",
    "                                           output=True,\n",
    "                                           frames_per_buffer=self.chunk,\n",
    "                                           input_device_index=5) #5- work\n",
    "        match audio:\n",
    "            case list():\n",
    "                for frame in audio:\n",
    "                    output_stream.write(frame)\n",
    "            case _:\n",
    "                output_stream.write(audio.tobytes())\n",
    "\n",
    "    def play_default(self):\n",
    "        output_stream = self.recorder.open(format=self.format,\n",
    "                                           channels=self.channels,\n",
    "                                           rate=self.rate,\n",
    "                                           output=True,\n",
    "                                           frames_per_buffer=self.chunk,\n",
    "                                           input_device_index=5) #5- work\n",
    "        with threading.Lock():\n",
    "            to_play = list(self.frames)\n",
    "        for frame in to_play:\n",
    "            output_stream.write(frame)\n",
    "    \n",
    "    def get(self):\n",
    "        with threading.Lock():\n",
    "            return list(self.frames)\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        self.input_stream.stop_stream()\n",
    "        self.input_stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\snakers4_silero-models_master\n",
      "E:\\anaconda\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:958: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_2:  0:00:01.958723\n",
      "Full recorded text:  ESSE JAE NOT KNOWING WHET\n",
      "end_3:  0:00:00.008266\n",
      "end_2:  0:00:00.201824\n",
      "Full recorded text:  YES SE JANE E NOT KNOWING WHET WE GURS MAKING LAL TAME SIE WILL SAY JUST I ID E\n",
      "end_3:  0:00:00.010520\n",
      "end_2:  0:00:00.189306\n",
      "Full recorded text:  ES SE JANE NOT KNOWING WHET WIGORS MAKING MALL TANEVYSIE WILL SAY JUST IIT IDIT YOUDON'T WAYS KNOW OUR PEATUR WY I SO\n",
      "end_3:  0:00:01.754830\n",
      "[ES we JANE NOT KNOWING WHET rigors MAKING MALL TANEVYSIE WILL SAY JUST it idiot YOUDON'T WAYS KNOW, OUR peanut, we, I, SO]\n",
      "end_4:  0:00:01.027821\n",
      "Мы, Дженни, не знаем, какие строгие требования делают БОЛЬШИЕ ТАНЕВИСИИ, СКАЗАТЬ: \"Ты просто идиот, что ты не знаешь\".\n",
      "end_5:  0:00:07.182376\n",
      "end_2:  0:00:00.271984\n",
      "Full recorded text:  HE SHAR BY HI TAR AN HE WAS REE THOSTIN HER AND LIKS LOR IN HIS OLD BESIDIN WE HAVE THE PLAN THAT THE SALO WAS GOING TO BE AWAY FOR PEOPL\n",
      "end_3:  0:00:00.986574\n",
      "[HE share BY HI TAR, AN HE WAS see hosting HER AND like for IN HIS OLD besides WE HAVE THE PLAN THAT THE salt WAS GOING TO BE AWAY FOR people]\n",
      "end_4:  0:00:00.285265\n",
      "Он делится с И. ТАР\n",
      "end_5:  0:00:15.138026\n",
      "end_2:  0:00:00.348454\n",
      "Full recorded text:  HE WAS RE THOUGHTS BE HERAND WIT MORINIS ONL BE FIGTING WE HAD TA PLAN THAT THE SALO WAS GOING TO BE AWAY FOR PEOPLE TO SEE WAR\n",
      "end_3:  0:00:00.518953\n",
      "end_2:  0:00:00.326081\n",
      "Full recorded text:  HE WAS RE THOUGHTS BE HERAND WIT MORINIS ONL BE FIGTING WE HAD TA PLAN THAT THE SALO WAS GOING TO BE AWAY FOR PEOPLE TO SEE WAR\n",
      "end_3:  0:00:00.502869\n",
      "[HE WAS RE THOUGHTS BE herald WIT marines on BE fighting WE HAD TA PLAN THAT THE salt WAS GOING TO BE AWAY FOR PEOPLE TO SEE WAR]\n",
      "end_4:  0:00:01.393322\n",
      "ОН ДЕЙСТВИТЕЛЬНО НАДЕЕТСЯ, ЧТО БУДУТ ГРЕПОДОБНЫЕ ВИТ-Морские пехотинцы в бою. У нас был план, что соль будет уходить для того, чтобы люди увидели войну.\n",
      "end_5:  0:00:04.585707\n",
      "end_2:  0:00:00.257689\n",
      "Full recorded text:  OF THE REAL THINGS GOING ON LIKE OUR HAR A DAY BECAUSE UNO YOU DON'T SEE N TO THE VISI BUT THE PEOPLE WHO ARE HERE BEATERING AROUND LOVE THE SCHOOL WHAT THEY SEE AND SO AREA OF THE SONG\n",
      "end_3:  0:00:00.014949\n",
      "end_2:  0:00:00.324064\n",
      "Full recorded text:  OF THE REAL THINGS GOING ON LIKE OUR HAR A DAY BECAUSE UNO YOU DON'T SEE N TO THE VISI BUT THE PEOPLE WHO ARE HERE BEATERING AROUND LOVE THE SCHOOL WHAT THEY SEE AND SO AREA OF THE SONG\n",
      "end_3:  0:00:00.012580\n",
      "[OF THE REAL THINGS GOING ON LIKE OUR her A DAY BECAUSE no YOU DON'T SEE N TO THE visit BUT THE PEOPLE WHO ARE HERE bettering AROUND LOVE THE SCHOOL WHAT THEY SEE AND SO AREA OF THE SONG]\n",
      "end_4:  0:00:01.969596\n",
      "О НАСТОЯЩИХ ВЕЩАХ, КОТОРЫЕ ПРОИСХОДЯТ КАК НАША ЕЖЕДНЕВНО, ПОТОМУ ЧТО «НЕТ», ВЫ НЕ ВИДИТЕ СВОЕГО ВИЗИТА, А ЛЮДИ, КОТОРЫЕ ЗАНИМАЮТСЯ ЭЛЕМЕНТОМ УДАЛЯЮТСЯ ПОДОБНОЙ ШКОЛЕ И ТАКОЙ ОБЛАСТЬЮ ПЕСНИ.\n",
      "end_5:  0:00:05.495811\n",
      "end_2:  0:00:00.252847\n",
      "Full recorded text:  ELLS A PEOPLE CAN A ECINE GET A REAL PROPER FIELD WHAT'S GOING ON NOT JUST A SORT OF IMBUSIVELY SHOP WE DOESN'T BREAK HOROMIN THE BUSELY SHOTE SAID NIT WAS IT WAS JUST ALWE ASE TO\n",
      "end_3:  0:00:01.443108\n",
      "end_2:  0:00:00.237444\n",
      "Full recorded text:  ELLS A PEOPLE CAN A ECINE GET A REAL PROPER FIELD WHAT'S GOING ON NOT JUST A SORT OF IMBUSIVELY SHOP WE DOESN'T BREAK HOROMIN THE BUSELY SHOTE SAID NIT WAS IT WAS JUST ALWE ASE TO\n",
      "end_3:  0:00:01.489722\n",
      "[ELLS A PEOPLE CAN A cine GET A REAL PROPER FIELD WHAT'S GOING ON NOT JUST A SORT OF impulsively SHOP WE DOESN'T BREAK heroin THE busily SHOTE SAID NIT WAS IT WAS JUST awe are TO]\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.audio = AudioChanel()\n",
    "        self.device = torch.device('cuda')\n",
    "        #you may use your models instead\n",
    "        self.processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(self.device)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "        self.spell = SpellChecker()\n",
    "        self.nlp = spacy.load('en_core_web_sm')#en_core_web_trf\n",
    "        self.PASS_VAR = False\n",
    "\n",
    "        #the worst model EVER\n",
    "        model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'# small, large and base\n",
    "        self.model_translate = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer_translate = T5Tokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        language = 'ru'\n",
    "        model_id = 'v4_ru'\n",
    "        self.sample_rate = 48000\n",
    "        self.speaker = 'kseniya'\n",
    "        \n",
    "        self.model_voice, _ = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                             model='silero_tts',\n",
    "                                             language=language,\n",
    "                                             speaker=model_id)\n",
    "        self.model_voice.to(self.device)\n",
    "        self.to_play = []\n",
    "    \n",
    "    def correcting(self,sent):\n",
    "        to_spell = sent.split()\n",
    "        corrected_words = [self.spell.correction(word) for word in to_spell]\n",
    "        return ' '.join((word if word is not None else to_spell[idx]) for idx, word in enumerate(corrected_words))\n",
    "\n",
    "    def send_to_play(self):\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "            with threading.Lock():\n",
    "                copy_to_play = list(self.to_play)\n",
    "            while len(copy_to_play):\n",
    "                to_play = copy_to_play[0]\n",
    "                translation = threading.Thread(target=self.audio.play,args=(to_play[0],))\n",
    "                original = threading.Thread(target=self.audio.play,args=(to_play[1],))\n",
    "                \n",
    "                translation.start()\n",
    "                original.start()\n",
    "                \n",
    "                translation.join()\n",
    "                original.join()\n",
    "                with threading.Lock():\n",
    "                    del self.to_play[0]\n",
    "                    del copy_to_play[0]\n",
    "    \n",
    "    async def run(self):\n",
    "        #start recording\n",
    "        _ = threading.Thread(target=self.audio.record).start()\n",
    "        _ = threading.Thread(target=self.send_to_play).start()\n",
    "        \n",
    "        # await asyncio.sleep(10)\n",
    "        last_state = ''\n",
    "        while True:\n",
    "            '''\n",
    "            PART 1\n",
    "            RECORDING\n",
    "            description: record every 2 seconds of audio from PC\n",
    "            '''\n",
    "            await asyncio.sleep(2)\n",
    "            time = datetime.now()\n",
    "            frames = self.audio.get()[:200]\n",
    "            to_model = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "            end_1 = datetime.now()\n",
    "            # print('end_1: ', end_1)\n",
    "            '''\n",
    "            PART 2\n",
    "            CONVERTING AUDIO TO TEXT\n",
    "            description: put audio into model and get recognized words\n",
    "            '''\n",
    "            input_values = self.processor(to_model, sampling_rate = self.audio.rate, return_tensors=\"pt\").input_values  # Batch size 1\n",
    "            logits = self.model(input_values.to(torch.float).to(self.device)).logits\n",
    "            predicted_ids = torch.argmax(logits, axis=-1)\n",
    "            \n",
    "            outputs = self.tokenizer.decode(predicted_ids[0], output_word_offsets=True)\n",
    "            # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n",
    "            time_offset = self.model.config.inputs_to_logits_ratio / self.feature_extractor.sampling_rate\n",
    "            \n",
    "            word_offsets = [\n",
    "                {\n",
    "                    \"word\": d[\"word\"],\n",
    "                    \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n",
    "                    \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n",
    "                }\n",
    "                for d in outputs.word_offsets\n",
    "            ]\n",
    "            end_2 = datetime.now()\n",
    "            print('end_2: ', end_2-end_1)\n",
    "            '''\n",
    "            PART 3\n",
    "            FIND SENTENCIES IN THE OUTPUT TEXT\n",
    "            description: find grammar parts in output text for building whole sentences\n",
    "            '''\n",
    "            text = ' '.join([word['word'] for word in word_offsets])\n",
    "            fixed = self.correcting(text)\n",
    "            print('Full recorded text: ', text)\n",
    "            if last_state == '':\n",
    "                last_state = text\n",
    "            else:\n",
    "                if last_state == text:\n",
    "                    self.PASS_VAR = True\n",
    "                last_state = text\n",
    "                \n",
    "            doc = self.nlp(fixed)\n",
    "    \n",
    "            end_3 = datetime.now()\n",
    "            print('end_3: ', end_3-end_2)\n",
    "            '''\n",
    "            PART 4\n",
    "            TRANSLATE SENTENCE INTO RUSSIAN (or any other language)\n",
    "            description: translate one of the list, because we should be sure about full context\n",
    "            '''\n",
    "            sents = list(doc.sents)\n",
    "            if len(sents)<=1 and not self.PASS_VAR:\n",
    "                continue\n",
    "            self.PASS_VAR = False\n",
    "            print(sents)\n",
    "            \n",
    "            prefix = 'translate to ru: '\n",
    "            ### delete part of audio.frames list\n",
    "            with threading.Lock():\n",
    "                idx_del = len(sents[0].text.split())-1 if len(sents[0].text.split())==len(word_offsets) else len(sents[0].text.split())\n",
    "                to_del = int(word_offsets[idx_del]['end_time']*self.audio.rate/self.audio.chunk)\n",
    "                play_background = self.audio.frames[:to_del+1]\n",
    "                del self.audio.frames[:to_del]\n",
    "            ###\n",
    "            src_text = prefix + sents[0].text\n",
    "            \n",
    "            # translate Englosh to Russian\n",
    "            input_ids = self.tokenizer_translate(src_text, return_tensors=\"pt\")\n",
    "            \n",
    "            generated_tokens = self.model_translate.generate(**input_ids.to(self.device))\n",
    "            \n",
    "            result = self.tokenizer_translate.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            end_4 = datetime.now()\n",
    "            print('end_4: ', end_4-end_3)\n",
    "            '''\n",
    "            PART 5\n",
    "            VOICEOVER OF TEXT AND PLAY\n",
    "            description: voice translated text\n",
    "            '''\n",
    "            print(result[0])\n",
    "            if not len(result[0]):\n",
    "                continue\n",
    "                  \n",
    "            audio = self.model_voice.apply_tts(text=result[0],\n",
    "                        speaker=self.speaker,\n",
    "                        sample_rate=self.sample_rate)\n",
    "            \n",
    "            audio_numpy = audio.cpu().numpy()\n",
    "            audio_resampled = librosa.resample(audio_numpy, orig_sr=self.sample_rate, \n",
    "                                               target_sr=self.audio.rate, res_typestr='fft')\n",
    "            audio_int16 = (audio_resampled * 32767).astype(np.int16)\n",
    "            end_5 = datetime.now()\n",
    "            print('end_5: ', end_5-end_4)\n",
    "            # self.audio.play(audio_int16)\n",
    "            with threading.Lock():\n",
    "                self.to_play.append((audio_int16,frames))\n",
    "            # _ = threading.Thread(target=self.audio.play, args=(play_background,)).start()\n",
    "            \n",
    "a = Agent()\n",
    "await a.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
