{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pyaudio\n",
    "import librosa\n",
    "import asyncio\n",
    "import threading\n",
    "#import deepspeech\n",
    "import wave\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoProcessor, AutoModelForCTC, TFWav2Vec2ForCTC, \\\n",
    "                         AutoTokenizer, AutoFeatureExtractor, T5ForConditionalGeneration, T5Tokenizer\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "#from autocorrect import Speller\n",
    "import openai\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРЕДИСЛОВИЕ\n",
    "\n",
    "#### Перед тем, как запускать прогу, необходимо:\n",
    "\n",
    "1) установить virtual audio cable\n",
    "\n",
    "2) в sound mixer options для всех бырать defaul input = Linme 1(virtual audio cable)\n",
    "\n",
    "3) затем для нужной программы (окна) в oputput выбрать тот же кабель (Line 1, например) \n",
    "\n",
    "4) вызвать код для нахождения подключённых аудиоустройств: ```for i in range(a.recorder.get_device_count()):\n",
    "       print(i, a.recorder.get_device_info_by_index(i))```\n",
    "\n",
    "5) выбрать подходящее аудиоустройство (для записи должно быть устройство Line 1, у которого ```'maxInputChannels' != 0```,\n",
    "  а для вывода должно быть устройство Line 1, у которого ```'maxOutputChannels' != 0```\n",
    "\n",
    "6) и когда открываете потоки в kwargs передаёте нужный вам индекс в переменную ```input_device_index```\n",
    "\n",
    "7) после запуска у нас звук будет перенаправляться так: приложение->Line_1_output->Line_1_input->Наша прога."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioChanel():\n",
    "    def __init__(self):\n",
    "        #change this according to your model\n",
    "        self.chunk = 1024\n",
    "        self.format = pyaudio.paInt16\n",
    "        self.channels = 1#2\n",
    "        self.rate = 16000\n",
    "        self.recorder = pyaudio.PyAudio()\n",
    "        self.frames = []\n",
    "        self.seconds = 2\n",
    "        # Открыть поток для записи\n",
    "        #1 {'index': 1, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 2, \n",
    "        #   'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, \n",
    "        #   'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
    "        self.input_stream = self.recorder.open(format=self.format,\n",
    "                                          channels=self.channels,\n",
    "                                          rate=self.rate,\n",
    "                                          input=True,\n",
    "                                          frames_per_buffer=self.chunk,\n",
    "                                          input_device_index=1) #0,1 - work\n",
    "        # Открыть поток для воспроизведения\n",
    "        #5 {'index': 5, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 0, \n",
    "        #   'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, \n",
    "        #   'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
    "        self.output_stream = self.recorder.open(format=self.format,\n",
    "                                           channels=self.channels,\n",
    "                                           rate=self.rate,\n",
    "                                           output=True,\n",
    "                                           frames_per_buffer=self.chunk,\n",
    "                                           input_device_index=5) #5- work\n",
    "        \n",
    "        \n",
    "    def record(self):\n",
    "        while True:\n",
    "            data = self.input_stream.read(self.chunk)\n",
    "            self.frames.append(data)\n",
    "\n",
    "    def play(self, audio = None):\n",
    "        output_stream = self.recorder.open(format=self.format,\n",
    "                                           channels=self.channels,\n",
    "                                           rate=self.rate,\n",
    "                                           output=True,\n",
    "                                           frames_per_buffer=self.chunk,\n",
    "                                           input_device_index=5) #5- work\n",
    "        match audio:\n",
    "            case list():\n",
    "                for frame in audio:\n",
    "                    output_stream.write(frame)\n",
    "            case _:\n",
    "                output_stream.write(audio.tobytes())\n",
    "\n",
    "    def play_default(self):\n",
    "        output_stream = self.recorder.open(format=self.format,\n",
    "                                           channels=self.channels,\n",
    "                                           rate=self.rate,\n",
    "                                           output=True,\n",
    "                                           frames_per_buffer=self.chunk,\n",
    "                                           input_device_index=5) #5- work\n",
    "        with threading.Lock():\n",
    "            to_play = list(self.frames)\n",
    "        for frame in to_play:\n",
    "            output_stream.write(frame)\n",
    "    \n",
    "    def get(self):\n",
    "        with threading.Lock():\n",
    "            return list(self.frames)\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        self.input_stream.stop_stream()\n",
    "        self.input_stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3272317480.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 44\u001b[1;36m\u001b[0m\n\u001b[1;33m    _ = threading.Thread(target=self.audio.play,args=(to_play[0],).start()\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.audio = AudioChanel()\n",
    "        self.device = torch.device('cuda')\n",
    "        #you may use your models instead\n",
    "        self.processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(self.device)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "        self.spell = SpellChecker()\n",
    "        self.nlp = spacy.load('en_core_web_sm')#en_core_web_trf\n",
    "        self.PASS_VAR = False\n",
    "\n",
    "        #the worst model EVER\n",
    "        model_name = 'utrobinmv/t5_translate_en_ru_zh_small_1024'# small, large and base\n",
    "        self.model_translate = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer_translate = T5Tokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        language = 'ru'\n",
    "        model_id = 'v4_ru'\n",
    "        self.sample_rate = 48000\n",
    "        self.speaker = 'kseniya'\n",
    "        \n",
    "        self.model_voice, _ = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                             model='silero_tts',\n",
    "                                             language=language,\n",
    "                                             speaker=model_id)\n",
    "        self.model_voice.to(self.device)\n",
    "        self.to_play = []\n",
    "    \n",
    "    def correcting(self,sent):\n",
    "        to_spell = sent.split()\n",
    "        corrected_words = [self.spell.correction(word) for word in to_spell]\n",
    "        return ' '.join((word if word is not None else to_spell[idx]) for idx, word in enumerate(corrected_words))\n",
    "\n",
    "    def send_to_play(self):\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "            with threading.Lock():\n",
    "                copy_to_play = list(self.to_play)\n",
    "            while len(copy_to_play):\n",
    "                to_play = copy_to_play[0]\n",
    "                _ = threading.Thread(target=self.audio.play,args=(to_play[0],).start()\n",
    "                _ = threading.Thread(target=self.audio.play,args=(to_play[1],).start()\n",
    "                with threading.Lock():\n",
    "                    del self.to_play[0]\n",
    "                    del copy_to_play[0]\n",
    "    \n",
    "    async def run(self):\n",
    "        #start recording\n",
    "        _ = threading.Thread(target=self.audio.record).start()\n",
    "        _ = threading.Thread(target=self.send_to_play).start()\n",
    "        \n",
    "        # await asyncio.sleep(10)\n",
    "        last_state = ''\n",
    "        while True:\n",
    "            '''\n",
    "            PART 1\n",
    "            RECORDING\n",
    "            description: record every 2 seconds of audio from PC\n",
    "            '''\n",
    "            await asyncio.sleep(2)\n",
    "            time = datetime.now()\n",
    "            frames = self.audio.get()[:200]\n",
    "            to_model = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "            end_1 = datetime.now()\n",
    "            # print('end_1: ', end_1)\n",
    "            '''\n",
    "            PART 2\n",
    "            CONVERTING AUDIO TO TEXT\n",
    "            description: put audio into model and get recognized words\n",
    "            '''\n",
    "            input_values = self.processor(to_model, sampling_rate = self.audio.rate, return_tensors=\"pt\").input_values  # Batch size 1\n",
    "            logits = self.model(input_values.to(torch.float).to(self.device)).logits\n",
    "            predicted_ids = torch.argmax(logits, axis=-1)\n",
    "            \n",
    "            outputs = self.tokenizer.decode(predicted_ids[0], output_word_offsets=True)\n",
    "            # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n",
    "            time_offset = self.model.config.inputs_to_logits_ratio / self.feature_extractor.sampling_rate\n",
    "            \n",
    "            word_offsets = [\n",
    "                {\n",
    "                    \"word\": d[\"word\"],\n",
    "                    \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n",
    "                    \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n",
    "                }\n",
    "                for d in outputs.word_offsets\n",
    "            ]\n",
    "            end_2 = datetime.now()\n",
    "            print('end_2: ', end_2-end_1)\n",
    "            '''\n",
    "            PART 3\n",
    "            FIND SENTENCIES IN THE OUTPUT TEXT\n",
    "            description: find grammar parts in output text for building whole sentences\n",
    "            '''\n",
    "            text = ' '.join([word['word'] for word in word_offsets])\n",
    "            fixed = self.correcting(text)\n",
    "            print('Full recorded text: ', text)\n",
    "            if last_state == '':\n",
    "                last_state = text\n",
    "            else:\n",
    "                if last_state == text:\n",
    "                    self.PASS_VAR = True\n",
    "                last_state = text\n",
    "                \n",
    "            doc = self.nlp(fixed)\n",
    "    \n",
    "            end_3 = datetime.now()\n",
    "            print('end_3: ', end_3-end_2)\n",
    "            '''\n",
    "            PART 4\n",
    "            TRANSLATE SENTENCE INTO RUSSIAN (or any other language)\n",
    "            description: translate one of the list, because we should be sure about full context\n",
    "            '''\n",
    "            sents = list(doc.sents)\n",
    "            if len(sents)<=1 and not self.PASS_VAR:\n",
    "                continue\n",
    "            self.PASS_VAR = False\n",
    "            print(sents)\n",
    "            \n",
    "            prefix = 'translate to ru: '\n",
    "            ### delete part of audio.frames list\n",
    "            with threading.Lock():\n",
    "                idx_del = len(sents[0].text.split())-1 if len(sents[0].text.split())==len(word_offsets) else len(sents[0].text.split())\n",
    "                to_del = int(word_offsets[idx_del]['end_time']*self.audio.rate/self.audio.chunk)\n",
    "                play_background = self.audio.frames[:to_del+1]\n",
    "                del self.audio.frames[:to_del]\n",
    "            ###\n",
    "            src_text = prefix + sents[0].text\n",
    "            \n",
    "            # translate Englosh to Russian\n",
    "            input_ids = self.tokenizer_translate(src_text, return_tensors=\"pt\")\n",
    "            \n",
    "            generated_tokens = self.model_translate.generate(**input_ids.to(self.device))\n",
    "            \n",
    "            result = self.tokenizer_translate.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            end_4 = datetime.now()\n",
    "            print('end_4: ', end_4-end_3)\n",
    "            '''\n",
    "            PART 5\n",
    "            VOICEOVER OF TEXT AND PLAY\n",
    "            description: voice translated text\n",
    "            '''\n",
    "            print(result[0])\n",
    "            if not len(result[0]):\n",
    "                continue\n",
    "                  \n",
    "            audio = self.model_voice.apply_tts(text=result[0],\n",
    "                        speaker=self.speaker,\n",
    "                        sample_rate=self.sample_rate)\n",
    "            \n",
    "            audio_numpy = audio.cpu().numpy()\n",
    "            audio_resampled = librosa.resample(audio_numpy, orig_sr=self.sample_rate, \n",
    "                                               target_sr=self.audio.rate, res_typestr='fft')\n",
    "            audio_int16 = (audio_resampled * 32767).astype(np.int16)\n",
    "            end_5 = datetime.now()\n",
    "            print('end_5: ', end_5-end_4)\n",
    "            # self.audio.play(audio_int16)\n",
    "            with threading.Lock():\n",
    "                self.to_play.append((audio_int16,frames))\n",
    "            # _ = threading.Thread(target=self.audio.play, args=(play_background,)).start()\n",
    "            \n",
    "a = Agent()\n",
    "await a.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to('cuda')\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "await asyncio.sleep(10)\n",
    "l = a.audio.get() #данные из потока pyaudio\n",
    "to_model = np.frombuffer(b''.join(l), dtype=np.int16)\n",
    "result = pipe(l)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'utrobinmv/t5_translate_en_ru_zh_large_1024'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to('cuda')\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Что-то здесь происходит, бугзапуск']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\snakers4_silero-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-5.3106e-03, -7.0865e-03, -7.2111e-03,  ...,  2.8435e-05,\n",
       "         9.6688e-06, -7.1262e-06])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "# misspelled = spell.unknown(['something', 'is', 'hapenning', 'here','booglaunch'])\n",
    "to_spell = ['something', 'is', 'hapenning', 'here','booglaunch']\n",
    "corrected_words = [spell.correction(word) for word in to_spell]\n",
    "fixed = ' '.join((word if word is not None else to_spell[idx]) for idx, word in enumerate(corrected_words))\n",
    "\n",
    "prefix = 'translate to ru: '\n",
    "src_text = prefix + fixed\n",
    "\n",
    "# translate Russian to Chinese\n",
    "input_ids = tokenizer(src_text, return_tensors=\"pt\")\n",
    "\n",
    "generated_tokens = model.generate(**input_ids.to('cuda'))\n",
    "\n",
    "result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(result)\n",
    "\n",
    "language = 'ru'\n",
    "model_id = 'v4_ru'\n",
    "sample_rate = 48000\n",
    "speaker = 'kseniya'\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model_voice, example_text = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                     model='silero_tts',\n",
    "                                     language=language,\n",
    "                                     speaker=model_id)\n",
    "model_voice.to(device)  # gpu or cpu\n",
    "\n",
    "audio = model_voice.apply_tts(text=result[0],\n",
    "                        speaker=speaker,\n",
    "                        sample_rate=sample_rate)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# Переводим тензор PyTorch в numpy массив\n",
    "audio_numpy = audio.cpu().numpy()\n",
    "audio_resampled = librosa.resample(audio_numpy, orig_sr=48000, target_sr=16000, res_typestr='fft')\n",
    "# Нормализуем данные и конвертируем их в формат int16 для pyaudio\n",
    "audio_int16 = (audio_resampled * 32767).astype(np.int16)\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Открытие потока для вывода\n",
    "stream = p.open(format=pyaudio.paInt16, \n",
    "                channels=1, \n",
    "                rate=16000, \n",
    "                output=True)\n",
    "\n",
    "# Воспроизведение аудио\n",
    "stream.write(audio_int16.tobytes())\n",
    "\n",
    "# Закрытие потока\n",
    "stream.stop_stream()\n",
    "stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cache for hugging face\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"E:\\cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"E:\\cache\"\n",
    "os.environ['HF_HOME'] = \"E:\\cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO когда санкции снимут\n",
    "# import os\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./google_cred.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученной модели и процессора\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at patrickvonplaten/wav2vec2-base-100h-with-lm were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at patrickvonplaten/wav2vec2-base-100h-with-lm and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
      "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade242dc484f4b1ea44cf905b0d3a536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='tokenizer_config.json', max=559.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--patrickvonplaten--wav2vec2-base-100h-with-lm. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac86b4f5044545c2bd89a3251fdeb28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='vocab.json', max=291.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61616c62db914af98903f10fbae63b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='special_tokens_map.json', max=85.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359b079eadaf4f2b95c09b4bb593db42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Fetching 4 files', max=4.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87958035ece4478093c1836367f590ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='alphabet.json', max=198.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\pyctcdecode\\models--patrickvonplaten--wav2vec2-base-100h-with-lm. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f60f26c58664348ba954dc62140c4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='language_model/unigrams.txt', max=1737592.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5642fec17254638ac89bd2261fd7227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='language_model/attrs.json', max=78.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dab5c05fd34e318a6e73a04ab2178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='4-gram.bin', max=3124591979.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kenlm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-46f7f15ff87f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_base\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForCTC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"patrickvonplaten/wav2vec2-base-100h-with-lm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprocessor_base\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"patrickvonplaten/wav2vec2-base-100h-with-lm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\auto\\processing_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m             )\n\u001b[0;32m    315\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mprocessor_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m             return processor_class.from_pretrained(\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\wav2vec2_with_lm\\processing_wav2vec2_with_lm.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mallow_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlanguage_model_filenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphabet_filename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             decoder = BeamSearchDecoderCTC.load_from_hf_hub(\n\u001b[0m\u001b[0;32m    162\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_patterns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_patterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyctcdecode\\decoder.py\u001b[0m in \u001b[0;36mload_from_hf_hub\u001b[1;34m(cls, model_id, cache_dir, **kwargs)\u001b[0m\n\u001b[0;32m    874\u001b[0m         )\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcached_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyctcdecode\\decoder.py\u001b[0m in \u001b[0;36mload_from_dir\u001b[1;34m(cls, filepath, unigram_encoding)\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[0mlanguage_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m             language_model = LanguageModel.load_from_dir(\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[0mfilenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"language_model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigram_encoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munigram_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyctcdecode\\language_model.py\u001b[0m in \u001b[0;36mload_from_dir\u001b[1;34m(cls, filepath, unigram_encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0munigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m         \u001b[0mkenlm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"kenlm\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkenlm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munigrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mjson_attrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kenlm' is not defined"
     ]
    }
   ],
   "source": [
    "model_base = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")\n",
    "processor_base = AutoProcessor.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efea5ed87e0b495eabb479c8f6a330bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\cache\\hub\\models--facebook--wav2vec2-base-960h. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd07a8eeac54b6986749bdd3341b06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d4a5b4f8154ce1b1ed2a784babfa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4212d0d275e64d37abb5215b576d82ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263c3e9a322d4efb8ae9f2de2910ee55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18204edb4ae4d53966ff76c575e1e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TFWav2Vec2ForCTC has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "All PyTorch model weights were used when initializing TFWav2Vec2ForCTC.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2ForCTC were not initialized from the PyTorch model and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor_l960 = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer_l960 = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model_l960 = TFWav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor_l960 = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'AH', 'start_time': 1.42, 'end_time': 1.5}, {'word': 'BUT', 'start_time': 1.98, 'end_time': 2.12}, {'word': 'INSTEAD', 'start_time': 2.62, 'end_time': 2.94}, {'word': 'OF', 'start_time': 3.0, 'end_time': 3.04}, {'word': 'MISS', 'start_time': 3.28, 'end_time': 3.44}, {'word': 'BUSSING', 'start_time': 3.48, 'end_time': 3.78}, {'word': 'CAUSE', 'start_time': 3.84, 'end_time': 4.0}, {'word': 'THIS', 'start_time': 4.1, 'end_time': 4.2}, {'word': 'IS', 'start_time': 4.28, 'end_time': 4.32}, {'word': 'A', 'start_time': 4.36, 'end_time': 4.38}, {'word': 'BOOGLAUNCH', 'start_time': 4.44, 'end_time': 4.86}, {'word': 'IS', 'start_time': 4.92, 'end_time': 5.0}, {'word': 'NOT', 'start_time': 5.04, 'end_time': 5.14}, {'word': 'ACTUALLY', 'start_time': 5.22, 'end_time': 5.44}, {'word': 'AT', 'start_time': 5.5, 'end_time': 5.54}, {'word': 'ALL', 'start_time': 5.68, 'end_time': 5.78}, {'word': 'WITH', 'start_time': 5.82, 'end_time': 5.9}, {'word': 'UTS', 'start_time': 5.96, 'end_time': 6.02}, {'word': 'OF', 'start_time': 6.06, 'end_time': 6.1}, {'word': 'THINGS', 'start_time': 6.14, 'end_time': 6.28}, {'word': 'I', 'start_time': 6.48, 'end_time': 6.5}, {'word': 'WILL', 'start_time': 6.54, 'end_time': 6.64}, {'word': 'SAVE', 'start_time': 6.68, 'end_time': 6.8}, {'word': 'THAT', 'start_time': 6.84, 'end_time': 6.94}, {'word': 'FHE', 'start_time': 7.0, 'end_time': 7.06}, {'word': 'NEXT', 'start_time': 7.12, 'end_time': 7.24}, {'word': 'SHE', 'start_time': 7.28, 'end_time': 7.36}, {'word': 'IS', 'start_time': 7.44, 'end_time': 7.52}, {'word': 'TALL', 'start_time': 7.56, 'end_time': 7.78}, {'word': 'TO', 'start_time': 7.98, 'end_time': 8.06}, {'word': 'SEE', 'start_time': 8.12, 'end_time': 8.22}, {'word': 'YOU', 'start_time': 8.24, 'end_time': 8.3}, {'word': 'THERE', 'start_time': 8.34, 'end_time': 8.46}, {'word': \"HE'S\", 'start_time': 8.78, 'end_time': 8.92}, {'word': 'GOING', 'start_time': 8.94, 'end_time': 9.12}]\n",
      "Full recorded text:  AH BUT INSTEAD OF MISS BUSSING CAUSE THIS IS A BOOGLAUNCH IS NOT ACTUALLY AT ALL WITH UTS OF THINGS I WILL SAVE THAT FHE NEXT SHE IS TALL TO SEE YOU THERE HE'S GOING\n",
      "Sentence: AH BUT INSTEAD OF MISS BUSSING CAUSE THIS IS A BOOGLAUNCH IS NOT ACTUALLY AT ALL WITH UTS OF THINGS I WILL SAVE THAT FHE\n",
      "Sentence: NEXT SHE IS TALL TO SEE YOU THERE\n",
      "Sentence: HE'S GOING\n"
     ]
    }
   ],
   "source": [
    "async def main(processor, model, tokenizer, feature_extractor, nlp):\n",
    "    a = AudioChanel()\n",
    "    frames = await a.recording()\n",
    "    to_model = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "    \n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    input_values = processor(to_model, sampling_rate = a.rate, return_tensors=\"tf\").input_values  # Batch size 1\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = tensorflow.argmax(logits, axis=-1)\n",
    "\n",
    "    # transcription = processor.decode(predicted_ids[0])\n",
    "    # print(\"Transcription:\", type(transcription), transcription)\n",
    "    \n",
    "    outputs = tokenizer.decode(predicted_ids[0], output_word_offsets=True)\n",
    "    # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n",
    "    time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate\n",
    "    \n",
    "    word_offsets = [\n",
    "        {\n",
    "            \"word\": d[\"word\"],\n",
    "            \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n",
    "            \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n",
    "        }\n",
    "        for d in outputs.word_offsets\n",
    "    ]\n",
    "    print(word_offsets)\n",
    "\n",
    "    text = ' '.join([word['word'] for word in word_offsets])\n",
    "    print('Full recorded text: ', text)\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        print(\"Sentence:\", sent.text)\n",
    "\n",
    "#     outputs = processor.decode(logits, output_word_offsets=True)\n",
    "#     time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate\n",
    "#     word_offsets = [\n",
    "#         {\n",
    "#             \"word\": d[\"word\"],\n",
    "#             \"start_time\": round(d[\"start_offset\"] * time_offset, 2),\n",
    "#             \"end_time\": round(d[\"end_offset\"] * time_offset, 2),\n",
    "#         }\n",
    "#         for d in outputs.word_offsets\n",
    "#     ]\n",
    "#     print(word_offsets)\n",
    "#     model_path = './deepspeech-0.9.3-models.pbmm'\n",
    "#     model = deepspeech.Model(model_path)\n",
    "#     print(model.stt(to_model))\n",
    "    \n",
    "    a.close()\n",
    "    \n",
    "# await main(processor, model)\n",
    "await main(processor_l960, model_l960, tokenizer_l960, feature_extractor_l960, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ac635624004bda94207fe7704047b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\cache\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f6d8c460ac41ba938e77ea3a001040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a41cfb5e644c07a431c711b190d010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d145c507c442dbbec5d8182131acec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85143442edc74ccc956c3299ccf0945d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68066500538c482d9fd1920df9db1c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "class TextCorrector:\n",
    "    def __init__(self, model_name='facebook/bart-large-cnn'):\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.model.eval()  # Перевести модель в режим оценки\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.to('cuda')\n",
    "\n",
    "    def correct_text(self, text):\n",
    "        inputs = self.tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(inputs, max_length=1024, num_beams=4, early_stopping=True)\n",
    "        corrected_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return corrected_text\n",
    "\n",
    "# Инициализация корректора текста\n",
    "corrector = TextCorrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "E:\\anaconda\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5372b617d8d44068ac16b45447233b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  61%|######1   | 545M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\cache\\hub\\models--prithivida--grammar_error_correcter_v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgramformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Gramformer\n\u001b[0;32m      2\u001b[0m grammar_correction \u001b[38;5;241m=\u001b[39m Gramformer(models \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m phrases \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m phrases:\n\u001b[0;32m      5\u001b[0m   corrections \u001b[38;5;241m=\u001b[39m grammar_correction\u001b[38;5;241m.\u001b[39mcorrect(phrase, max_candidates\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from gramformer import Gramformer\n",
    "grammar_correction = Gramformer(models = 1, use_gpu=True)\n",
    "phrases = [text.lower()]\n",
    "for phrase in phrases:\n",
    "  corrections = grammar_correction.correct(phrase, max_candidates=2)\n",
    "  print(f'[Incorrect phrase] {phrase}')\n",
    "  for i in range(len(corrections)):\n",
    "    print(f'[Suggestion #{i}] {corrections[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Incorrect phrase] ah but instead of miss bussing cause this is a booglaunch is not actually at all with uts of things i will save that fhe next she is tall to see you there he's going\n",
      "[Suggestion #] ah but instead of miss bussing cause this is a booglaunch is not actually at all with uts of things i will save that fhe next she is tall to see you there he's going to be able to see it.]\n",
      "[Suggestion #] ah but instead of miss bussing cause this is a booglaunch is not actually at all with uts of things i will save that fhe next she is tall to see you there he's going to be able to see if he']\n",
      "[Suggestion #] ah but instead of miss bussing cause this is a booglaunch is not actually at all with uts of things i will save that fhe next she is tall to see you there he's going to be able to see.]\n",
      "[Suggestion #] ah but instead of miss bussing cause this is a booglaunch is not actually at all with uts of things i will save that fhe next she is tall to see you there he's going to be able to see if i']\n",
      "[Suggestion #] ah but instead of miss bussing cause this is a booglaunch is not actually at all with uts of things i will save that fhe next she is tall to see you there he's going to be a hit.]\n"
     ]
    }
   ],
   "source": [
    "text = \"AH BUT INSTEAD OF MISS BUSSING CAUSE THIS IS A BOOGLAUNCH IS NOT ACTUALLY AT ALL WITH UTS OF THINGS I WILL SAVE THAT FHE NEXT SHE IS TALL TO SEE YOU THERE HE'S GOING\"\n",
    "phrases = [text.lower()]\n",
    "# phrases = ['BOOGLAUNCH it is a book launch']\n",
    "\n",
    "for phrase in phrases:\n",
    "    corrections = grammar_correction.correct(phrase, max_candidates=5)\n",
    "    print(f'[Incorrect phrase] {phrase}')\n",
    "    for i in corrections:\n",
    "        print(f'[Suggestion #] {i}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i hate machine learning. i hate machinelearning. I hatemachine learning. I love machine learning, but I hate it when it tries to make me do something I don't want to do. I. hate machineLearning. I Hate Machine Learning. I Love Machine Learning, I Love You.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"AH BUT INSTEAD OF MISS BUSSING CAUSE THIS IS A BOOGLAUNCH IS NOT ACTUALLY AT ALL WITH UTS OF THINGS I WILL SAVE THAT FHE NEXT SHE IS TALL TO SEE YOU THERE HE'S GOING\"\n",
    "text_accumulated = \"translate text into russian: \" + text.lower()\n",
    "corrector.correct_text('i hate machine learning')\n",
    "# inputs = corrector.tokenizer('say hi', return_tensors=\"pt\", max_length=512, truncation=True).to('cuda')\n",
    "    \n",
    "# # Получение предсказаний от модели\n",
    "# with torch.no_grad():\n",
    "#     outputs = corrector.model(**inputs)\n",
    "\n",
    "# # Получаем логиты токенов\n",
    "# logits = outputs.logits\n",
    "\n",
    "# # Выбираем самые вероятные токены\n",
    "# predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# # Преобразуем токены обратно в текст\n",
    "# corrected_text = corrector.tokenizer.decode(predicted_tokens[0])\n",
    "# print(corrected_text)\n",
    "# Отправка накопленного текста на коррекцию\n",
    "# corrected_text = corrector.correct_text(text_accumulated)\n",
    "# print(\"Corrected Text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"BOOGLAUNCH is an acronym that means you can easily find information about everything in a single word with no words, and there's a great tutorial for\"}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation',model='gpt2-large')\n",
    "set_seed(42)\n",
    "generator(\"BOOGLAUNCH is\", max_length=32, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# This is the default and can be omitted\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk-proj-dopNSK7YOpa21ceNqujsT3BlbkFJNi5shx98CkjOayPoGuM1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      9\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     10\u001b[0m         {\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay this is a test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m         }\n\u001b[0;32m     14\u001b[0m     ],\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-16k\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32mE:\\anaconda\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:640\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    638\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    639\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    642\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    643\u001b[0m             {\n\u001b[0;32m    644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    645\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    646\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    647\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    648\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    667\u001b[0m             },\n\u001b[0;32m    668\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    669\u001b[0m         ),\n\u001b[0;32m    670\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    671\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    672\u001b[0m         ),\n\u001b[0;32m    673\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    674\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    676\u001b[0m     )\n",
      "File \u001b[1;32mE:\\anaconda\\Lib\\site-packages\\openai\\_base_client.py:1250\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1238\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1247\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1248\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1249\u001b[0m     )\n\u001b[1;32m-> 1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mE:\\anaconda\\Lib\\site-packages\\openai\\_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    932\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    933\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    934\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    935\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    936\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    937\u001b[0m     )\n",
      "File \u001b[1;32mE:\\anaconda\\Lib\\site-packages\\openai\\_base_client.py:1030\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1029\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1033\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1034\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1038\u001b[0m )\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key='w',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'index': 0, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Input', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "1 {'index': 1, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "2 {'index': 2, 'structVersion': 2, 'name': 'РњРёРєСЂРѕС„РѕРЅ (Realtek(R) Audio)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "3 {'index': 3, 'structVersion': 2, 'name': 'Line 2 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "4 {'index': 4, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Output', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "5 {'index': 5, 'structVersion': 2, 'name': 'Р”РёРЅР°РјРёРєРё (Realtek(R) Audio)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "6 {'index': 6, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "7 {'index': 7, 'structVersion': 2, 'name': 'Line 2 (Virtual Audio Cable)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "8 {'index': 8, 'structVersion': 2, 'name': 'Primary Sound Capture Driver', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "9 {'index': 9, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "10 {'index': 10, 'structVersion': 2, 'name': 'РњРёРєСЂРѕС„РѕРЅ (Realtek(R) Audio)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "11 {'index': 11, 'structVersion': 2, 'name': 'Line 2 (Virtual Audio Cable)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "12 {'index': 12, 'structVersion': 2, 'name': 'Primary Sound Driver', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "13 {'index': 13, 'structVersion': 2, 'name': 'Р”РёРЅР°РјРёРєРё (Realtek(R) Audio)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "14 {'index': 14, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "15 {'index': 15, 'structVersion': 2, 'name': 'Line 2 (Virtual Audio Cable)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "16 {'index': 16, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 2, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.003, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.01, 'defaultSampleRate': 48000.0}\n",
      "17 {'index': 17, 'structVersion': 2, 'name': 'Р”РёРЅР°РјРёРєРё (Realtek(R) Audio)', 'hostApi': 2, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.003, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.01, 'defaultSampleRate': 48000.0}\n",
      "18 {'index': 18, 'structVersion': 2, 'name': 'Line 2 (Virtual Audio Cable)', 'hostApi': 2, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.003, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.01, 'defaultSampleRate': 48000.0}\n",
      "19 {'index': 19, 'structVersion': 2, 'name': 'РњРёРєСЂРѕС„РѕРЅ (Realtek(R) Audio)', 'hostApi': 2, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.003, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.01, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 48000.0}\n",
      "20 {'index': 20, 'structVersion': 2, 'name': 'Line 1 (Virtual Audio Cable)', 'hostApi': 2, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.003, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.01, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 48000.0}\n",
      "21 {'index': 21, 'structVersion': 2, 'name': 'Line 2 (Virtual Audio Cable)', 'hostApi': 2, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.003, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.01, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 48000.0}\n",
      "22 {'index': 22, 'structVersion': 2, 'name': 'Р“РѕР»РѕРІРЅРѕР№ С‚РµР»РµС„РѕРЅ (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(Redmi AirDots_R))', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 1, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 8000.0}\n",
      "23 {'index': 23, 'structVersion': 2, 'name': 'Р“РѕР»РѕРІРЅРѕР№ С‚РµР»РµС„РѕРЅ (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(Redmi AirDots_R))', 'hostApi': 3, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 8000.0}\n",
      "24 {'index': 24, 'structVersion': 2, 'name': 'Speakers (Realtek HD Audio output)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 8, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "25 {'index': 25, 'structVersion': 2, 'name': 'РЎС‚РµСЂРµРѕ РјРёРєС€РµСЂ (Realtek HD Audio Stereo input)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "26 {'index': 26, 'structVersion': 2, 'name': 'РњРёРєСЂРѕС„РѕРЅ (Realtek HD Audio Mic input)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "27 {'index': 27, 'structVersion': 2, 'name': 'Р“РѕР»РѕРІРЅРѕР№ С‚РµР»РµС„РѕРЅ (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(Mi True Wireless EBs Basic S))', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 1, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 8000.0}\n",
      "28 {'index': 28, 'structVersion': 2, 'name': 'Р“РѕР»РѕРІРЅРѕР№ С‚РµР»РµС„РѕРЅ (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(Mi True Wireless EBs Basic S))', 'hostApi': 3, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 8000.0}\n",
      "29 {'index': 29, 'structVersion': 2, 'name': 'РќР°СѓС€РЅРёРєРё ()', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n",
      "30 {'index': 30, 'structVersion': 2, 'name': 'Р“РѕР»РѕРІРЅРѕР№ С‚РµР»РµС„РѕРЅ (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(MOMENTUM 4))', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 1, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 8000.0}\n",
      "31 {'index': 31, 'structVersion': 2, 'name': 'Р“РѕР»РѕРІРЅРѕР№ С‚РµР»РµС„РѕРЅ (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(MOMENTUM 4))', 'hostApi': 3, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 8000.0}\n",
      "32 {'index': 32, 'structVersion': 2, 'name': 'Line Out (Virtual Cable 1)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "33 {'index': 33, 'structVersion': 2, 'name': 'Line Out (Virtual Cable 2)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "34 {'index': 34, 'structVersion': 2, 'name': 'Mic 1 (Virtual Cable 1)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "35 {'index': 35, 'structVersion': 2, 'name': 'Line 1 (Virtual Cable 1)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "36 {'index': 36, 'structVersion': 2, 'name': 'S/PDIF 1 (Virtual Cable 1)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "37 {'index': 37, 'structVersion': 2, 'name': 'Mic 2 (Virtual Cable 2)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "38 {'index': 38, 'structVersion': 2, 'name': 'Line 2 (Virtual Cable 2)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "39 {'index': 39, 'structVersion': 2, 'name': 'S/PDIF 2 (Virtual Cable 2)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "40 {'index': 40, 'structVersion': 2, 'name': 'РќР°СѓС€РЅРёРєРё ()', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n",
      "41 {'index': 41, 'structVersion': 2, 'name': 'РќР°СѓС€РЅРёРєРё ()', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.08533333333333333, 'defaultHighOutputLatency': 0.08533333333333333, 'defaultSampleRate': 44100.0}\n"
     ]
    }
   ],
   "source": [
    "a = AudioChanel()\n",
    "for i in range(a.recorder.get_device_count()):\n",
    "    print(i, a.recorder.get_device_info_by_index(i))#['name'].encode('latin1', errors='ignore').decode('cp1251'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
